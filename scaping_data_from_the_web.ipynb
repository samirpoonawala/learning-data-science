{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scaping data from the web.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/samirpoonawala/learning-data-science/blob/master/scaping_data_from_the_web.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "eA7j-wv3_4Co",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (Treehouse) Scaping Data from the Web\n",
        "[course link](https://teamtreehouse.com/library/scraping-data-from-the-web)\n",
        "\n",
        "Almost any information you want is available on the Internet. Web scraping is a key tool for data mining that information allowing for web page exploration and collection for a variety of reporting. The tools and techniques used in this course allow for data to be collected that would otherwise not be easily accessible without robotic assistance.\n",
        "\n",
        "What you'll learn:\n",
        "\n",
        "\n",
        "*   An introduciton to the Beautiful Soup Python package\n",
        "*   How to scrape a web page with Beautiful Soup\n",
        "*   An introduction to the Scrapy Python package\n",
        "*   How to crawl a website with Scrapy\n",
        "*   Web scraping considerations\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3nGKGDLYwIpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introducing Data Scraping\n",
        "\n",
        "A look at what data scraping is and how it is used. We'll have a discussion about how a web page is designed and look at the Python package, Beautiful Soup to scrape data from the web."
      ]
    },
    {
      "metadata": {
        "id": "_FQ7bDlD_WZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is data scraping?\n",
        "A high-level overview of the world of data web scraping in Python. What it is and isn't and how it can be used.\n",
        "\n",
        "\n",
        "*  Web scraping is the automated collecting of data from the web by any means other than a program interacting with a web API"
      ]
    },
    {
      "metadata": {
        "id": "NGMX0v9HAMBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Web page anatomy\n",
        "\n",
        "Let's take a brief look at how an HTML page is structured so we can better understand how to navigate a page for web scraping\n",
        "\n",
        "\n",
        "*   [House Land](https://treehouse-projects.github.io/horse-land/index.html) website\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "v4DKhMVFBk2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Beautiful Soup\n",
        "Introducing the Python web scraping package, Beautiful Soup"
      ]
    },
    {
      "metadata": {
        "id": "qc1AkdOxBtUq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d6w5BmKCCjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib2 import urlopen\n",
        "# Python 3 command:\n",
        "#from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen(\"https://treehouse-projects.github.io/horse-land/index.html\")\n",
        "soup = BeautifulSoup(html.read(), 'html.parser')\n",
        "\n",
        "# print out page\n",
        "#print(soup.prettify())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1w8KSD9CmPa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print page title\n",
        "print(soup.title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lnXzVFWD67l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# find all divs on the website\n",
        "divs = soup.find_all('div')\n",
        "for div in divs:\n",
        "  print(div)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1xKpbniEKCM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# filter divs by class\n",
        "featured_divs = soup.find_all('div', {'class':'featured'})\n",
        "for featured_div in featured_divs:\n",
        "  print(featured_div)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eHwdyJErEpSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## More Soup in the Tureen\n",
        "Let's look at two Beautiful Soup methods, 'find()' and 'find_all()', in greater detail"
      ]
    },
    {
      "metadata": {
        "id": "JA8-U915E6s0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use find to find first instance of an item\n",
        "div = soup.find('div', {'class':'featured'})\n",
        "print(div)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrwwzjwWFK6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# chaining elements together\n",
        "featured_header = soup.find('div', {'class':'featured'}).h2\n",
        "print(featured_header)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "npMkinnFFgL-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# just the text (w/o h2 tag)\n",
        "# use as last step of the scraping process (harder to work with during scraping process)\n",
        "featured_header = soup.find('div', {'class':'featured'}).h2\n",
        "print(featured_header.get_text())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FE8AjeImGDkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print all references to primary button class\n",
        "\n",
        "for button in soup.find(attrs={'class':'button button--primary'}):\n",
        "  print(button)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QYzCkIjIH0uN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shortcut for above\n",
        "for button in soup.find(class='button button--primary'):\n",
        "  print(button)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_QYSaF4HAqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get all hyperlinks on a specific page\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "  print(link.get('href'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ompmqtw-It_P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Being a Good Citizen\n",
        "Just because we can do something doesn't mean we always should. Let's take a look at some of the responsibilities taht come with the power of web scraping.\n",
        "\n",
        "*   Web scraping legal claims (USA): copyright infringement; computer fraud and abuse act (CFAA); tresspass to chattels\n",
        "*   EU: Directive 96/9/EC (Database Directive);\n",
        "*   Austrailia: Spam Act of 2003\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EctYn65rJ0QR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Everyone Loves Charlotte\n",
        "We've seen how to scrape data from a single page. Now let's see how we can capture links on one page and follow them to process additional pages."
      ]
    },
    {
      "metadata": {
        "id": "kfEbH49UKADt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib2 import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "\n",
        "site_links = []\n",
        "\n",
        "def internal_links(linkURL):\n",
        "  html = urlopen('https://treehouse-projects.github.io/horse-land/{}'.format(linkURL))\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  \n",
        "  return soup.find('a', href=re.compile('(.html)$'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  urls = internal_links('index.html')\n",
        "  while len(urls) > 0:\n",
        "    page = urls.attrs['href']\n",
        "    if page not in site_links:\n",
        "      site_links.append(page)\n",
        "\n",
        "      print(page)\n",
        "\n",
        "      print('\\n==================\\n')\n",
        "\n",
        "      urls = internal_links(page)\n",
        "    else:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rt0iZjQHNCpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing Scrapy\n",
        "Getting up and going with the Scrapy library"
      ]
    },
    {
      "metadata": {
        "id": "0ae_1eq1MMHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install scrapy\n",
        "!pip install scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcsDMNWtNSyG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# scrapy project setup: terminal command\n",
        "!scrapy startproject AraneaSpider"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnjChPq6Nz_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd /usr/local/lib/python2.7/dist-packages/scrapy/templates/project"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0L4i9fEoOEdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d2as5cQoOZ_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Crawling Spiders\n",
        "\n",
        "Let's use the Python Library, Scrapy, to create a spider to crawl the web"
      ]
    },
    {
      "metadata": {
        "id": "bNcykBV5OGwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# new file to crawl site\n",
        "import scrapy\n",
        "\n",
        "class HorseSpider(scrapy.Spider):\n",
        "  name = 'ike'\n",
        "  def start_requests(self):\n",
        "    urls = ['https://treehouse-projects.github.io/horse-land/index.html', \n",
        "            'https://treehouse-projects.github.io/horse-land/mustang.html']\n",
        "    \n",
        "    return [scrapy.Request(url=url, callback=self.parse) for url in urls]\n",
        "  \n",
        "  def parse(self, response):\n",
        "    url = response.url\n",
        "    page = url.split('/')[-1]\n",
        "    filename = 'horses-%s' % page\n",
        "    print('URL is {}'.format(url))\n",
        "    with open(filename, 'wb') as file:\n",
        "      file.write(response.body)\n",
        "    print(\"Saved file %s\" % filename)\n",
        "    \n",
        "    \n",
        "# NOTE: getting error: 'no module named zope.interface'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xG0GTGgeTQ7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tried using this to fix error in cell above\n",
        "!pip install zope"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9ktfLCDQRn3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# execute script above in terminal\n",
        "# after navigating file directory to spiders folder\n",
        "\n",
        "scrapy crawl ike"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3yL2JqnUMOl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Endless Web\n",
        "Let's further explore how to crawl the web\n",
        "\n",
        "\n",
        "*   First scraper scrapped a static list of URLs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8WLWK_0OUyeY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# crawler.py\n",
        "\n",
        "# imports\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "\n",
        "class HorseSpider(CrawlSpider):\n",
        "  \n",
        "  name = 'Whirlaway'\n",
        "  \n",
        "  allowed_domains = ['treehouse-projects.github.io']\n",
        "  \n",
        "  start_urls = ['http://treehouse-projects.github.io/horse-land']\n",
        "  \n",
        "  rules = [Rule(LinkExtractor(allow=r'.*).\n",
        "                             callback='parse_horses',\n",
        "                             follow=True)]\n",
        "  \n",
        "  def parse_horses(self, response):\n",
        "                url = response.url\n",
        "                title = response.css('title::text').extract()\n",
        "                print(\"Page URL: {}\".format(url))\n",
        "                print(\"Page title: {}\".format(title))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnLuXAfuVSKy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!scrapy crawl whirlaway"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kHhMgmgeDqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Additional Scraping Tasks\n",
        "Going beyond static web pages can be a challenge when scraping. Working with web forms and APIs can require a different approach. We'll also touch on how to write tests for a web scraper.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LEzZGwVae3Th",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## An Intelligent Scraper\n",
        "\n",
        "Forms are a big part of many websites. Scrapy provides a FormRequest class for handling them."
      ]
    },
    {
      "metadata": {
        "id": "BANrH6ITeELy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# formSpider.py\n",
        "\n",
        "from scrapy.http import FormRequest\n",
        "from scrapy.spiders import Spider\n",
        "\n",
        "class FormSpider(Spider):\n",
        "  name = 'horseForm'\n",
        "  start_urls = [\"https//treehouse-projects.github.io/horse-land/form.html\"]\n",
        "  \n",
        "  def parse(self, response):\n",
        "    formdata = {'firstname':'Samir',\n",
        "               'lastname': \"Poonawala\",\n",
        "               \"title\": \"Partner and Chief Financial Officer\"}\n",
        "    \n",
        "    return FormRequest.form_response(response, formnumber=0, \n",
        "                                     formdata=formdata, callback-self.after_post)\n",
        "  \n",
        "  def after_post(self, response):\n",
        "    print(\"\\n\\n******\\nForm processed.\\n\")\n",
        "    print(response)\n",
        "    print(\"\\n******\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3IcotU-wvikJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!scrapy crawl horseForm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyvaSVQqvpQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scraping APIs\n",
        "\n",
        "APIs are all around us on the web. Sometimes we can use scraping techniques to interact with them in a meaningful way."
      ]
    },
    {
      "metadata": {
        "id": "iW5PsEYhvrOz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#world_bank.py\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def get_country(country_code):\n",
        "  html = urlopen(\"http://api.worldbank.org/v2/countries/{}\".format(country_code))\n",
        "  \n",
        "  soup = BeautifulSoup(html, 'xml')\n",
        "  \n",
        "  country_name = soup.find('wb:name')\n",
        "  region = soup.find('wb:region')\n",
        "  income_level = soup.find('wb:incomelevel')\n",
        "  \n",
        "  print(country_name.get_text())\n",
        "  print(region.get_text())\n",
        "  print(incomelevel.get_text())\n",
        "  \n",
        " if __name__ == \"__main__\":\n",
        "  # references csv file included in project files with course\n",
        "  file = open(\"country_iso_codes.csv\", \"r\")\n",
        "  iso_codes = csv.reader(file, delimiter = ',')\n",
        "  \n",
        "  for code in iso_codes:\n",
        "    get_country(code[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcJHIonRyxEo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using scrapers for site testing\n",
        "\n",
        "Web scraping doesn't have to entirely be about scraping data for processing. Web scraping tools can be used to test websites as well."
      ]
    },
    {
      "metadata": {
        "id": "t-QSxlvZyxYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#horse_test.py\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import unittest\n",
        "\n",
        "class TestHorseLand(unittest.TestCase):\n",
        "  soup = None\n",
        "  \n",
        "  def setUpClass():\n",
        "    url = \"https://treehouse-projects.github.io/horse-land/index.html\"\n",
        "    TestHorseLand.soup = BeautifulSoup(urlopen(url), \"html.parser\")\n",
        "    \n",
        "  def test_header1(self):\n",
        "    header1 = TestHorseLand.soup.find('h1').get_text()\n",
        "    self.assertEqual(\"Horse Land\", header1)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "  unittest.main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rx5NINUy1HgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# horse_test_selenium\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "\n",
        "import time\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "driver.get(\"https://treehouse-projects.github.io/horse-land/index.html\")\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "page_html = driver.page_source\n",
        "\n",
        "soup = BeautifulSoup(page_html, 'html.parser')\n",
        "\n",
        "print(soup.prettify())\n",
        "\n",
        "driver.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PW2IgX3v24Xl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Common Issues with Data Scraping\n",
        "Challenges in Data Scraping\n",
        "We've discussed a few of the challenges already. Topics such as bot access and legal crawling can pose hurdles to scraping data.\n",
        "\n",
        "Other things to watch out for that can pose hurdles or outright walls to your data scraping include:\n",
        "\n",
        "User Authentication & Captchas\n",
        "Honeypots\n",
        "Structural site changes\n",
        "IP blocking\n",
        "Latency\n",
        "We've seen that JavaScript poses challenges itself, but dynamic websites in general pose challenges. Especially those that utilize AJAX mechanisms.\n",
        "\n",
        "Potential Solutions\n",
        "User Authentication can be handled similar to forms and Scrapy has a loginform library to help with these tasks.\n",
        "Captchas can be worked around with various technologies, but can still severely slow down the scraping process.\n",
        "Site changes force web scraping developers to keep up to date with the \"target\" sites and may require the spiders and scraping tools to be rewritten to account for site changes.\n",
        "One way around IP blocking is to utilize multiple IP addresses for your scraping efforts.\n",
        "\n",
        "Other thoughts\n",
        "Some of the things to think about in terms of how to best handle scraping websites are:\n",
        "\n",
        "Be polite and honest about your scraping intentions.\n",
        "\n",
        "Minimize the load on a single website that you visit for scraping. \n",
        "\n",
        "Scraping can put a heavy load on their web servers. One technique to handle this is to cache the pages you crawl so that you don't have to load them again.\n",
        "\n",
        "Make your efforts as inconspicuous as possible to reduce suspicion from target websites."
      ]
    },
    {
      "metadata": {
        "id": "q2nzSmza2qbs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping Up\n",
        "\n"
      ]
    }
  ]
}